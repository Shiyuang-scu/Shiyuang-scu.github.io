---
title: ã€å¼ºåŒ–å­¦ä¹ ã€‘Policy Gradient
date: 2018-11-03 19:50:36
tags: ä¸–ç•Œä¹‹å†…
---

2018.11.3åˆ›å»º

2018.12.9æ›´æ–°

2018.12.10æ›´æ–°

2018.12.11å®Œç¨¿

æœ¬ç¯‡ä»‹ç»äº†Policy Gradientçš„ç»“æ„ã€ç®—æ³•ï¼Œä»¥åŠä¼˜åŒ–

<!--more-->

**Q ï¼šPolicy Gradientå’ŒQ-Learningçš„åŒºåˆ«åœ¨äºï¼Œå‰è€…ç›´æ¥é€‰æ‹©åŠ¨ä½œï¼Œåè€…æ ¹æ®å¥–æƒ©å€¼é€‰æ‹©åŠ¨ä½œã€‚Q-Learningçš„æ–¹æ³•ï¼Œé€šä¿—ç‚¹æ¥è¯´ï¼Œå°±æ˜¯å“ªä¸ªåŠ¨ä½œæ›´æœ‰å¥½å¤„ï¼Œå°±é€‰æ‹©å“ªä¸ªã€‚ä½†æ˜¯ï¼Œ**PGåœ¨é€‰æ‹©åŠ¨ä½œä¸Šçš„ç­–ç•¥åœ¨æœ¬è´¨ä¸Šä¸ä¹Ÿæ˜¯å› ä¸ºè¿™ä¸ªåŠ¨ä½œæ›´æœ‰å¥½å¤„å—ï¼Ÿæ‰€ä»¥ä¸è¿˜æ˜¯éœ€è¦ä¸€ä¸ªç±»ä¼¼å¥–æƒ©å€¼æ¥ä¾›å‚è€ƒå—ï¼Ÿ

**Aï¼š**ç¬¬ä¸€å¥è¯çš„ååŠå¥è¯´å¾—æ²¡é”™ï¼Œä½†å‰åŠå¥å°±é”™äº†ã€‚PGå’ŒQLçš„åŒºåˆ«åœ¨äºï¼ŒQLæ˜¯åŸºäºè´ªå©ªç®—æ³•çš„ï¼Œå…·ä½“åœ°è¯´ï¼Œå°±æ˜¯å°†æ¯ä¸€ä¸ªç›®æ ‡åˆ†æˆä¸€ä¸ªä¸ªå°ç›®æ ‡ï¼Œåªè¦æŠŠå°ç›®æ ‡åšå¾—å¥½ï¼Œå¤§ç›®æ ‡å°±åšå¥½äº†ï¼Œæ‰€ä»¥QLåšçš„æ˜¯ä¼˜åŒ–æ¯ä¸€ä¸ªå°ç›®æ ‡ï¼Œè€Œä¸ç®¡å¤§ç›®æ ‡æ€ä¹ˆæ ·ï¼›PGåˆ™å€’è¿‡æ¥ï¼Œå®ƒä¸ç®¡å°ç›®æ ‡æ€ä¹ˆæ ·ï¼Œä»–åªå…³å¿ƒå¤§ç›®æ ‡â€”â€”åªè¦å¤§ç›®æ ‡å®Œæˆå¥½äº†ï¼Œä¹‹åå°±å¤šå¤šç”¨å®Œæˆå¤§ç›®æ ‡çš„è¿™ç§æ–¹æ³•ï¼Œå¦‚æœå¤§ç›®æ ‡å®Œæˆä¸å¥½ï¼Œé‚£è¿™ç§æ–¹æ³•å°±ç»™å®ƒæ‰“å…¥å†·å®«ã€‚



# *ä¸ºä»€ä¹ˆæ˜¯PGï¼Œè€Œä¸æ˜¯QLï¼Ÿ*

QLçš„è¾“å…¥æ˜¯åŠ¨ä½œå’ŒçŠ¶æ€ï¼Œè¾“å‡ºçš„æ˜¯å¥–åŠ±å€¼ï¼ˆè¯„åˆ¤åœ¨è¿™ç§çŠ¶æ€ä¸‹åšå‡ºè¿™ç§åŠ¨ä½œæ˜¯å¥½æ˜¯åçš„åˆ†æ•°ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦æŠŠæ‰€æœ‰åœ¨è¯¥çŠ¶æ€ä¸‹å¯èƒ½çš„åŠ¨ä½œåšä¸€éå¾—åˆ°æ¯ä¸ªåŠ¨ä½œçš„å¥–åŠ±å€¼ï¼Œç„¶åé€‰æ‹©å¥–åŠ±å€¼æœ€å¤§çš„ï¼ˆåœ¨è¯¥çŠ¶æ€ä¸‹æœ€æœ‰åˆ©çš„åŠ¨ä½œï¼‰ã€‚å› ä¸ºè¿™ç§æ€§è´¨ï¼Œæ‰€ä»¥QLç®—æ³•å¾—åˆ°çš„ç­–ç•¥ä¸å…·æœ‰æ•´ä½“è§‚â€”â€”å› ä¸ºå®ƒé€‰æ‹©çš„æ˜¯æ¯ä¸€ä¸ªå¯¹å½“å‰çŠ¶æ€æœ€æœ‰åˆ©çš„è¡Œä¸ºã€‚æ‰€ä»¥æœ¬è´¨ä¸ŠQLä¹Ÿå°±æ˜¯è´ªå©ªç®—æ³•ã€‚

PGçš„è¾“å…¥æ˜¯çŠ¶æ€ï¼Œè¾“å‡ºæ˜¯åŠ¨ä½œï¼Œæ˜¯ç«¯å¯¹ç«¯çš„ç®—æ³•ï¼Œæ›´åŠ ç›´è§‚è€Œä¸”æ›´å…·æœ‰æ•´ä½“æ€§ã€‚PGå°±æ˜¯å…ˆä¸ç®¡ä¸‰ä¸ƒäºŒåä¸€ï¼Œéšæœºæ¥ä¸ª100å¥—åŠ¨ä½œè¯•ä¸€è¯•ï¼Œå…¶ä¸­æœ‰çš„ä¸€å¥—åšå¾—å¥½ï¼Œæœ‰çš„åšå¾—ä¸å¥½ï¼Œè¿™ä¸€å¥—åŠ¨ä½œåšå¾—å¥½ï¼Œå°±åŠ åˆ†ï¼Œå¢åŠ ä¸‹æ¬¡åšè¿™å¥—åŠ¨ä½œçš„å¯èƒ½æ€§ï¼›é‚£ä¸€å¥—åŠ¨ä½œåšå¾—ä¸å¥½ï¼Œå°±æ‰£åˆ†ï¼Œä¸‹æ¬¡å°±å°½å¯èƒ½ä¸åšè¿™å¥—åŠ¨ä½œäº†ã€‚è¿™æ ·ä¸‹æ¥ï¼Œè®­ç»ƒé‡å¤šäº†ï¼Œå°±å¤§æ¦‚çŸ¥é“ä»€ä¹ˆçŠ¶æ€ä¸‹è¯¥åšä»€ä¹ˆäº†ã€‚

æ­£å¦‚Karpathyåœ¨å…¶åšæ–‡ä¸­å†™çš„ï¼š

>It turns out that Q-Learning is not a great algorithm (you could say that DQN is so 2013 (okay Iâ€™m 50% joking)). In fact most people prefer to use Policy Gradients, including the authors of the original DQN paper who have [shown](http://arxiv.org/abs/1602.01783) Policy Gradients to work better than Q Learning when tuned well. 

è¿DQNçš„ä½œè€…éƒ½æ‰¿è®¤ï¼ŒPGæ¯”DQLè¡¨ç°å¾—æ›´å¥½ã€‚ğŸ˜

# Pong ä¸ºä¾‹

æˆ‘ä»¬ä»¥é›…è¾¾åˆ©2600ä¸Šçš„æ¸¸æˆPongä¸ºä¾‹ï¼Œæ¥å±•ç¤ºç®—æ³•çš„æ€æƒ³å’Œç»†èŠ‚ã€‚

![Pong](https://raw.githubusercontent.com/Shiyuang-scu/blog_img/master/policygradient-1.gif)

Pongå°±æ˜¯ä¸€ä¸ªç±»ä¼¼äºä¹’ä¹“çš„æ¸¸æˆï¼Œä½ è¦æ§åˆ¶å³è¾¹çš„æ¿å­ï¼ŒæŠŠå·¦è¾¹ç”µè„‘æ§åˆ¶çš„æ¿å­æ‰“è¿‡æ¥çš„çƒæ‰“è¿‡å»ï¼Œå¯¹æ–¹æ²¡æ¥åˆ°ä½ å¾—ä¸€åˆ†ï¼Œä½ æ²¡æ¥åˆ°å¯¹æ–¹å°±å¾—ä¸€åˆ†ã€‚æŠŠè¿™ä¸ªæ¸¸æˆæ‹†åˆ†æˆè®¡ç®—æœºèƒ½å¤Ÿæ‡‚å¾—çš„æ¨¡å‹ï¼Œå°±æ˜¯ï¼šæˆ‘ä»¬çœ‹åˆ°ä¸€å¸§å›¾ç‰‡(æ¯”å¦‚è¯´210\*160\*3çš„åƒç´ )ï¼Œé€šè¿‡åˆ†æå›¾ç‰‡ä¸­æ¿å­å’Œçƒçš„ä½ç½®å’Œè¿åŠ¨è¶‹åŠ¿ï¼Œæ¥åšå‡ºæ¿å­ç§»åŠ¨çš„å†³ç­–ï¼Œè¿™ä¸ªå†³ç­–åªæœ‰ä¸¤ä¸ªï¼šä¸Šï¼ˆUPï¼‰å’Œä¸‹ï¼ˆDOWNï¼‰ã€‚

# Policy Network 

é¦–å…ˆæˆ‘ä»¬è¦å®šä¹‰ä¸€ä¸ªå†³ç­–ç½‘ç»œ(Policy Network)ã€‚è¿™ä¸ªç½‘ç»œä¼šæ”¶åˆ°æ¸¸æˆçš„çŠ¶æ€ï¼Œç„¶åè¾“å‡ºåœ¨è¿™ä¸ªçŠ¶æ€ä¸‹åº”è¯¥åšçš„å†³ç­–ã€‚åœ¨Pongä¸­ï¼Œæˆ‘ä»¬çš„è¾“å…¥æ˜¯ä¸€å¸§å›¾ç‰‡ï¼Œè¾“å‡ºæ˜¯UP or DOWNï¼Œå› ä¸ºåªæœ‰ä¸¤ä¸ªè¡Œä¸ºï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åªç”¨ä¸€ä¸ªç¥ç»å…ƒå°±èƒ½è¡¨ç¤ºäº†ï¼ˆå½“ç„¶ä¹Ÿå¯ä»¥ç”¨ä¸¤ä¸ªï¼Œå¦‚æœç”¨ä¸€ä¸ªï¼Œé‚£ä¹ˆè¾“å‡ºå…¶å®æ˜¯åšå‡ºUPæˆ–è€…DOWNç­–ç•¥çš„æ¦‚ç‡ï¼‰ã€‚

![Policy Network](https://raw.githubusercontent.com/Shiyuang-scu/blog_img/master/policygradient-2.png)

è¿™æ˜¯æˆ‘ä»¬çš„è®¡ç®—ä»£ç ï¼š

```python
h = np.dot(W1, x) # compute hidden layer neuron activations
h[h<0] = 0 # ReLU nonlinearity: threshold at zero
logp = np.dot(W2, h) # compute log probability of going up
p = 1.0 / (1.0 + np.exp(-logp)) # sigmoid function (gives probability of going up)
```

ç›´è§‚ä¸Šæ¥è¯´ï¼Œéšå±‚ï¼ˆW1ï¼‰æå–äº†å›¾ç‰‡ä¸­çš„å„ä¸ªç‰¹å¾ï¼Œæ¯”å¦‚æ¿å­åœ¨å“ªå„¿ï¼Œçƒåœ¨å“ªå„¿ï¼Œè¾“å‡ºå±‚ï¼ˆW2ï¼‰åˆ™å†³å®šäº†UPå’ŒDOWNçš„æ¦‚ç‡ï¼Œæ‰€ä»¥ï¼Œåœ¨Policy Networkä¸­ï¼ŒW1å’ŒW2æ˜¯æœ€é‡è¦çš„å‚æ•°ï¼Œæˆ‘ä»¬è¦è®­ç»ƒçš„å°±æ˜¯è¿™ä¸¤ç»„å‚æ•°ã€‚

*ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬ä¼šç”¨è‡³å°‘ä¸¤å¸§å›¾ç‰‡ä½œä¸ºè¾“å…¥ï¼ˆæŠŠä¸¤å¸§å›¾åŠ èµ·æ¥æˆ–è€…å‡ä¸€ä¸‹ï¼Œæˆ–è€…å¹²è„†å¹¶æ’æ”¾åœ¨ä¸€èµ·ï¼‰ï¼Œä¸ºäº†ä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿè¯†åˆ«å‡ºè¿ç»­çš„åŠ¨ä½œã€‚*

--------

---------

**ç–‘é—®**ï¼šPolicy NetworkæŠ½è±¡ä¸Šæ¥è¯´ï¼Œå®é™…ä¸Šå°±æ˜¯æ ¹æ®ä¸€å¸§å¸§çš„å›¾åƒæ¥åé¦ˆç›¸å¯¹åº”çš„ä¸€ä¸ªä¸ªåŠ¨ä½œï¼Œé—®é¢˜åœ¨äºï¼Œä¹Ÿè®¸æˆ‘åˆ°ç¬¬20ä¸ªåŠ¨ä½œæ‰å¾—åˆ†ï¼Œä½†æ˜¯æˆ‘ä¹‹æ‰€ä»¥èƒ½å¾—åˆ†ï¼Œæ˜¯å–å†³äºæˆ‘ç¬¬10ä¸ªåŠ¨ä½œç²¾å¦™çš„æŠ€å·§ï¼Œä½†æ˜¯Policy Networkè‚¯å®šä¸çŸ¥é“å•Šï¼Œé‚£ä¹ˆæˆ‘ä»¬åˆè°ˆä½•å­¦ä¹ è¿™ä¸ªåŠ¨ä½œï¼Œå¢åŠ è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡å‘¢ï¼Ÿæ‰€ä»¥é—®é¢˜åœ¨äºï¼Œæˆ‘ä»¬è¦çŸ¥é“æ˜¯å“ªä¸€ä¸ªåŠ¨ä½œå†³å®šäº†æˆ‘å¾—åˆ†ï¼Œå¹¶ä¸”åœ¨æˆ‘çŸ¥é“è¿™ä¸ªå¾—åˆ†åå­¦ä¹ åˆ°äº†è¿™ä¸ªåŠ¨ä½œã€‚

# Supervised Learning   

å¦‚æœä½ ä¹‹å‰æ¥è§¦è¿‡ç›‘ç£å­¦ä¹ ï¼Œå°±ä¼šå‘ç°å†³ç­–ç½‘ç»œå’Œç›‘ç£å­¦ä¹ æ˜¯éå¸¸åƒçš„ï¼Œå”¯ä¸€ä¸€ç‚¹ä¸åŒå°±æ˜¯Labelï¼Œé‚£ä¹ˆæˆ‘ä»¬åœ¨å­¦ä¹ Policy Gradientä¹‹å‰ï¼Œå°±å…ˆè°ˆè°ˆç›‘ç£å­¦ä¹ (Supervised Learning)ã€‚

å¦‚ä¸‹å›¾ï¼Œæˆ‘ä»¬è¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼Œå¾—åˆ°äº†è¾“å‡ºï¼Œè¾“å‡ºçš„æ˜¯logæ¦‚ç‡ï¼ˆæ˜¯logæ¦‚ç‡è¿˜æ˜¯åŸå§‹æ¦‚ç‡ï¼Œä¸æ˜¯å…³é”®ï¼‰ã€‚åœ¨ç›‘ç£å­¦ä¹ ä¸‹ï¼Œæˆ‘ä»¬çŸ¥é“åœ¨è¿™ç§æƒ…å†µä¸‹åº”è¯¥æ€ä¹ˆåšï¼Œå‡è®¾æˆ‘ä»¬çŸ¥é“åº”è¯¥UPï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥ç”¨BPç®—æ³•æ¥æ›´æ–°æƒé‡ï¼Œæ¥è¾¾åˆ°å¢åŠ åœ¨è¿™ç§æƒ…å†µä¸‹å¾—å‡ºUPå†³ç­–çš„æ¦‚ç‡ã€‚



![Supervised Learning](https://raw.githubusercontent.com/Shiyuang-scu/blog_img/master/policygradient-3.png)

å…³äºç›‘ç£å­¦ä¹ çš„å†…å®¹ä¸å†èµ˜è¿°ï¼Œå¯ä»¥æŸ¥ç›¸å…³èµ„æ–™ã€‚

# Policy Gradient

é‚£ä¹ˆå¦‚æœæˆ‘ä»¬æ²¡æœ‰Labelæ€ä¹ˆåŠå‘¢ï¼Ÿå‡å¦‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€æ¬¡è®¡ç®—ï¼Œå¾—åˆ°UPçš„æ¦‚ç‡ä¸º30%ï¼ŒDOWNçš„æ¦‚ç‡ä¸º70%ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ä¼šåœ¨è¿™ä¸ªåˆ†å¸ƒä¸­éšæœºé€‰æ‹©ä¸€ä¸ªï¼Œå‡å¦‚é€‰ä¸­çš„æ˜¯DOWNå§ï¼Œé‚£ä¹ˆåœ¨è¿™æ—¶å°±è®©æ¿å­å¾€ä¸‹ç§»åŠ¨ï¼Œå¦‚æœè¿™ä¸ªç§»åŠ¨æ˜¯å¥½çš„ï¼Œé‚£ä¹ˆLabelå°±æ˜¯positiveçš„ï¼Œåœ¨backpropåå°±å¢åŠ åœ¨è¿™ä¸ªçŠ¶æ€ä¸‹DOWNçš„æ¦‚ç‡ã€‚é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬æ€ä¹ˆçŸ¥é“è¿™å°±æ˜¯å¥½çš„å‘¢ï¼Ÿæ¯”å¦‚æˆ‘åœ¨ç¬¬ä¸€å¸§é€‰æ‹©äº†DOWNï¼Œç„¶ååœ¨20å¸§æ‰èµ¢/è¾“äº†æ¸¸æˆï¼Œæˆ‘åˆä¸çŸ¥é“ç¬¬ä¸€å¸§çš„åŠ¨ä½œå¯¹ç»“æœæœ‰ä»€ä¹ˆå½±å“ã€‚

æˆ‘ä»¬å¯ä»¥è¿™ä¹ˆåšï¼Œå…ˆåšä¸€ç³»åˆ—åŠ¨ä½œï¼Œç›´åˆ°æ¸¸æˆç»“æŸï¼Œç„¶åå†æ ¹æ®ç»“æœï¼Œå¯¹è¿™ä¸€ä¸²åŠ¨ä½œä¸­çš„æ¯ä¸€ä¸ªåŠ¨ä½œæ ‡è®°Labelã€‚æ¯”å¦‚è¯´æˆ‘ç¬¬ä¸€å¸§DOWNï¼Œç„¶åæœ€åèµ¢äº†ï¼Œé‚£ä¹ˆDOWNçš„Labelå°±æ˜¯positiveçš„ã€‚

åœ¨PGä¸­ï¼Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°æ˜¯$\sum_iA_ilogp(y_i|x_i)$,å…¶ä¸­$A_i$æ˜¯**advantage**ï¼Œå¦‚æœèµ¢äº†å°±æ˜¯1ï¼Œè¾“äº†å°±æ˜¯-1ï¼›$y_i$æ˜¯æˆ‘ä»¬é€‰å–çš„åŠ¨ä½œï¼›$x_i$æ˜¯è¾“å…¥çš„çŠ¶æ€ï¼ˆåœ¨Pongä¾‹å­ä¸­æ˜¯å›¾ç‰‡ï¼ˆåƒç´ ï¼‰ï¼‰ã€‚

ä»¥ä¸Šä»¥pongä¸ºä¾‹ï¼Œå¯¹advantagçš„é€‰æ‹©ä¹Ÿè¿‡äºè‰ç‡ï¼ˆå› ä¸ºä¸€å¾‹éƒ½æ˜¯1æˆ–è€…0ï¼‰ï¼Œå®é™…ä¸Šï¼Œå¯¹äºä¸åŒçš„åŠ¨ä½œ$x$ï¼Œadvangeæ˜¯ä¸åŒçš„ã€‚

------------

-----------

ä¸‹é¢æˆ‘ä»¬æŠ›å»pongè¿™ä¸ªä¾‹å­ï¼Œæ¥é‡æ–°æè¿°policy gradientã€‚

## åŸºæœ¬ç»„ä»¶

å¯¹äºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè¦æ˜ç¡®ï¼Œè¿™ä¸ªé—®é¢˜çš„åŸºæœ¬å¯¹è±¡æœ‰è¿™äº›ï¼šActorã€Environmentã€Reward Functionã€‚

ä¸¾ä¸ªä¾‹å­ï¼ŒAlphaGoä¸‹å›´æ£‹çš„é—®é¢˜ï¼ŒActoræ˜¯AlphaGoï¼ŒEnvironmentæ˜¯å¯¹æ‰‹å’Œæ£‹ç›˜ï¼ŒReward FunctionåŸºäºå›´æ£‹çš„è§„åˆ™ã€‚å†ä¸¾ä¸ªæ —å­ï¼Œæˆ‘ä»¬ç©ç”µå­æ¸¸æˆï¼Œæ‰‹æŸ„æ˜¯Actorï¼Œç”µè„‘å±å¹•æˆ–è€…æ¸¸æˆæœºæ˜¯Environmentï¼ŒReward Functionæ˜¯æ€æ­»æ•Œäººæˆ–è€…å é¢†æ®ç‚¹ï¼ˆè¿˜æ˜¯ä¾æ®æ¸¸æˆè§„åˆ™ï¼‰ã€‚å…¶ä¸­Environmentå’ŒReward Functionæ˜¯æˆ‘ä»¬**æ— æ³•**åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¹å˜çš„ï¼Œåªæœ‰Actoræ˜¯**å—æ§**çš„ã€‚

é‚£ä¹ˆå¦‚ä½•æ§åˆ¶Actorï¼Ÿ

## Policy of Actor

æˆ‘ä»¬å¯¹Actorçš„æ§åˆ¶ï¼Œéœ€è¦ç­–ç•¥ï¼Œè€Œç­–ç•¥æˆ‘ä»¬ç”¨ $\pi$ ç¬¦å·è¡¨ç¤ºã€‚

$\pi$ æ˜¯ä¸€ä¸ªå‚æ•°ä¸º $\theta$ çš„ç¥ç»ç½‘ç»œï¼Œè¾“å…¥æ˜¯observationï¼ˆçŸ©é˜µï¼‰ï¼›è¾“å‡ºæ˜¯æ•°é‡ä¸º $n$ çš„ç¥ç»å…ƒã€‚

observationæ˜¯ç»™ç¥ç»ç½‘ç»œæä¾›å½“å‰çŠ¶æ€çš„å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ¸¸æˆçš„å½“å‰å›¾åƒä½œä¸ºobservationï¼Œè¿™æ—¶observationå®é™…ä¸Šæ˜¯ä¸€ä¸²åƒç´ ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥æŠŠå„ç§å‚æ•°ä½œä¸ºobservationï¼Œæ¯”å¦‚è¯´æˆ‘å’Œæ•Œäººçš„è·ç¦»ã€æˆ‘æ–¹è‹±é›„æŠ€èƒ½çš„æ”»å‡»èŒƒå›´ã€‚æ€»ä¹‹ï¼Œobservationæä¾›äº†å½“å‰çŠ¶æ€çš„ç‰¹å¾ã€‚

è¾“å‡ºçš„æ¯ä¸ªç¥ç»å…ƒä»£è¡¨ä¸€ä¸ªåŠ¨ä½œï¼Œç¥ç»å…ƒåŒ…å«çš„å€¼è¡¨ç¤ºè¿™ä¸ªåŠ¨ä½œçš„å‘ç”Ÿæ¦‚ç‡ã€‚

![image-20181209160020187](https://raw.githubusercontent.com/Shiyuang-scu/blog_img/master/policygradient-4.png)

æˆ‘ä»¬ä¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œï¼Œä½œä¸ºç­–ç•¥çš„æœ€ç»ˆç»“æœï¼Œç„¶åæ ¹æ®åé¦ˆï¼Œè®¡ç®—è¿™ä¸ªåŠ¨ä½œçš„å¾—åˆ†**reward**ï¼Œç¬¬$i$ä¸ªåŠ¨ä½œ$a_i$çš„rewardç”¨$r_i$è¡¨ç¤ºã€‚

## ä»¥ç”µè„‘æ¸¸æˆä¸ºä¾‹

æ¯”å¦‚è¯´æˆ‘ä»¬ç©ä¸€ä¸ªç”µè„‘æ¸¸æˆï¼Œæ¸¸æˆç°åœ¨çš„çŠ¶æ€æ˜¯$s_1$ï¼Œç„¶å $\pi$ é€‰æ‹©äº†åŠ¨ä½œ$a_1$ï¼Œæ¸¸æˆç…§åšäº†ï¼Œç„¶åæˆ‘ä»¬è®¡ç®—ä¸€ä¸ªreward $r_1$ï¼›æ¸¸æˆåœ¨æ‰§è¡Œäº†åŠ¨ä½œ$a_1$åçš„çŠ¶æ€ä¸º$s_2$ï¼Œç„¶åæˆ‘ä»¬å†é€‰æ‹©äº†åŠ¨ä½œ$a_2$ï¼Œæ‰§è¡ŒåŠ¨ä½œåï¼Œè®¡ç®—äº†ä¸€ä¸ªreward $r_2$ï¼Œè¿™æ ·ä¸€ç›´ä¸‹å»ï¼Œç›´åˆ°æ¸¸æˆç»“æŸï¼ˆæ¸¸æˆèµ¢äº†æˆ–è€…è¾“äº†ï¼‰ã€‚

æˆªæ­¢æ¸¸æˆç»“æŸï¼Œæˆ‘ä»¬ä¸€å…±é€‰æ‹©äº†$T$ä¸ªåŠ¨ä½œï¼Œè®¡ç®—å‡ºäº†$T$ä¸ªrewardï¼Œè¿™ä¸€å›åˆæˆ‘ä»¬ç§°ä¹‹ä¸ºä¸€ä¸ª**episode**ï¼Œä¸€ä¸ªepisodeçš„æ•´ä¸ªrewardæ˜¯æ¯ä¸ªåŠ¨ä½œçš„rewardä¹‹å’Œï¼š

 $R = \sum^T_{t=1}{r_t}$ 

æˆ‘ä»¬ç”¨ä¸€ä¸ªå›¾æ¥æ›´ç›´è§‚åœ°çœ‹çœ‹åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­Actor, Environment, Reward ä¹‹é—´çš„å…³ç³»ã€‚

![image-20181209161133065](https://raw.githubusercontent.com/Shiyuang-scu/blog_img/master/policygradient-5.png)

è¿™æ•´ä¸ªepisodeæ˜¯Environmentã€Actorä¹‹é—´çš„äº¤äº’ï¼Œåœ¨episodeä¸­äº§ç”Ÿäº†ä¸€ä¸²ç¯å¢ƒä¸åŠ¨ä½œçš„åºåˆ—ï¼Œå«åš**Trajectory**

 **Trajectory**   $\tau = \{s_1,a_1,s_2,a_2,...,s_T,a_T\}$

æˆ‘ä»¬å¯ä»¥ç®—å‡ºæ•´ä¸ªtrajectoryçš„æ¦‚ç‡$p_\theta(\tau)$

$p_\theta(\tau)=p(s_1)p_\theta(a_1|s_1)p(s_2|s_1,a_1)p_\theta(a_2|s_2)...=p(s_1)\Pi^T_{t=1}p_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)$

æœ‰ä¸€ç‚¹å¾ˆ**é‡è¦**ï¼Œè™½ç„¶åœ¨episodeä¸­ï¼Œ$\theta $æ˜¯ç¡®å®šçš„ï¼Œä½†æ˜¯Environmentæ˜¯ä¸å—æˆ‘ä»¬æ§åˆ¶çš„ï¼Œæ‰€ä»¥å³ä½¿åœ¨ç›¸åŒçš„$\theta$ä¸‹ï¼Œæ¸¸æˆçŠ¶æ€$s_i$æ˜¯ä¸å—æ§çš„ï¼Œæ‰€ä»¥åœ¨ä¸åŒçš„çŠ¶æ€ä¸‹ï¼Œäº§ç”Ÿå‡ºçš„trajectoryä¹Ÿä¼šä¸åŒã€‚æ‰€ä»¥è¦æ±‚åœ¨ $\theta$ ä¸‹çš„Rewardï¼Œæˆ‘ä»¬éœ€è¦æ±‚Rçš„æœŸæœ›Expectation

$\overline{R_\theta}=\sum_\tau{R(\tau)p_\theta{(\tau)}=E_{\tau{-}{p_\theta}}[R(\tau)]}$

æˆ‘ä»¬çš„ç›®çš„å¾ˆæ˜ç¡®å¾ˆç®€å•ï¼Œå°±æ˜¯è°ƒæ•´$\theta$æŠŠ$\overline{R_\theta}$æœ€å¤§åŒ–ï¼Œä¸‹é¢å°±æ˜¯æˆ‘ä»¬å¸¸ç”¨çš„æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰

$\theta \leftarrow \theta + {\eta} {\nabla} \overline{R_{\theta}}$

å…¶ä¸­

$\nabla\overline{R_\theta}=\sum_\tau{R(\tau){\nabla}p_\theta{(\tau)}}=E_{\tau{-}{p_\theta}}[R(\tau){\nabla}logp_{\theta}(\tau)]$

${\approx}\frac{1}{N}\sum^N_{n=1}R(\tau^n){\nabla}logp_{\theta}(\tau^n)$

$=\frac{1}{N}\sum^N_{n=1}\sum^{T_n}\_{t=1}R(\tau^n){\nabla}logp\_{\theta}(a^n_t|s^n_t)$

æ•´ä¸ªè¿‡ç¨‹å¦‚å›¾

![image-20181209164225978](https://raw.githubusercontent.com/Shiyuang-scu/blog_img/master/policygradient-6.png)

åœ¨$N$ä¸ªepisodeä¸­ï¼Œæˆ‘ä»¬è¦è®°å½•æ¯ä¸€ä¸ª$\tau^n$ä¸­çš„åºåˆ—å¯¹$(s^n_i,a^n_i)$ï¼Œè®°å½•$\tau^n$çš„reward$R({\tau}^n)$ï¼Œç„¶åå¯¹$\theta$è¿›è¡Œæ›´æ–°ã€‚

## Important Tips

åœ¨Policy Gradientä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€äº›ä¼˜åŒ–ï¼Œè¿™äº›ä¼˜åŒ–åŒ…æ‹¬ï¼š**å¢åŠ åŸºçº¿ã€åˆ†é…åˆé€‚å¥–åŠ±å€¼**

### Tip1: Add a Baseline

å¢åŠ åŸºçº¿ï¼Œå¾ˆå¥½ç†è§£ï¼Œæˆ‘ä»¬çœ‹ä¸‹é¢è¿™ä¸ªæƒ…å†µï¼š

åœ¨Policy Gradientä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸ä¼šå‘ç°åœ¨å®é™…æƒ…å†µä¸‹ï¼Œæ¯ä¸€ä¸ªåŠ¨ä½œçš„å¥–åŠ±å€¼éƒ½æ˜¯æ­£çš„ï¼Œæ¯”å¦‚è¯´æˆ‘ä»¬è®­ç»ƒAgentç©FIFAï¼Œå› ä¸ºAgentè¿›çš„çƒæœ€å°‘æ˜¯0ï¼Œå…¶å¥–åŠ±å€¼ä¸å¯èƒ½æ˜¯ä¸€ä¸ªè´Ÿæ•°ã€‚

ä½†æ˜¯å®é™…ä¸Šï¼Œå³ä½¿éƒ½æ˜¯æ­£çš„ï¼Œä¹Ÿ**ä¸ä¼šå½±å“**åˆ°æˆ‘ä»¬è®­ç»ƒçš„ç»“æœï¼Œå› ä¸ºå¥–åŠ±å€¼æœ‰å¤šæœ‰å°‘ï¼Œæ‰€ä»¥åœ¨å›é¦ˆçš„æ—¶å€™ï¼Œå¯¹å‚æ•°çš„è°ƒæ•´ä¹Ÿæœ‰å¤§æœ‰å°ã€‚å¥–åŠ±å€¼å¤§çš„ï¼Œå‚æ•°å°±å¾€è¿™ä¸€ä¸ªåŠ¨ä½œåºåˆ—é è¿‘å¾—å¤§ï¼›å¥–åŠ±å€¼å°çš„ï¼Œå‚æ•°å°±å¾€è¿™ä¸€ä¸ªåŠ¨ä½œåºåˆ—é è¿‘å¾—å°ã€‚å› ä¸ºè¾“å‡ºçš„åŠ¨ä½œæ˜¯æ ¹æ®åŠ¨ä½œçš„æ¦‚ç‡æ¥åˆ¤å®šçš„ï¼Œè€Œæ‰€æœ‰çš„åŠ¨ä½œæ¦‚ç‡ä¹‹å’Œä¸º1ï¼Œä¸€ä¸ªåŠ¨ä½œæ¦‚ç‡å¤§äº†ï¼Œå…¶ä»–åŠ¨ä½œçš„æ¦‚ç‡å°±ä¼šå˜å°ï¼Œæ‰€ä»¥å‚æ•°å€¾å‘äºå¯¹å¥–åŠ±å€¼å¤§çš„åŠ¨ä½œé è¿‘ï¼Œå®é™…ä¸Šå°±æ˜¯åœ¨è¿œç¦»å¥–åŠ±å€¼è¾ƒå°çš„åŠ¨ä½œã€‚

ä½†é—®é¢˜æ˜¯ï¼Œæ³¨æ„æˆ‘ä»¬ä¹‹å‰æ¨å¯¼çš„æ¢¯åº¦ä¸‹é™å…¬å¼ï¼š

$\nabla\overline{R_\theta}{\approx}\frac{1}{N}\sum^N_{n=1}\sum^{T_n}\_{t=1}R(\tau^n){\nabla}logp\_{\theta}(a^n_t|s^n_t)$

æˆ‘ä»¬æ˜¯åœ¨å¯¹åŠ¨ä½œåºåˆ— $\tau^{i}$ è¿›è¡Œé‡‡æ ·ï¼Œé‡‡æ ·çš„ç»“æœæ˜¯ï¼Œæˆ‘ä»¬ä¸å¯èƒ½æŠŠæ‰€æœ‰çš„å¯èƒ½åºåˆ—éƒ½é‡‡é›†åˆ°ã€‚æ‰€ä»¥ä¼šå‡ºç°è¿™ç§æƒ…å†µï¼šæœ‰ä¸€ä¸ªåŠ¨ä½œåºåˆ—${\tau}^{j}$çš„å¥–åŠ±å€¼æ˜¯å¾ˆé«˜çš„ï¼Œä½†æ˜¯æˆ‘ä»¬æ²¡æœ‰é‡‡æ ·åˆ°ï¼Œæˆ‘ä»¬åªé‡‡æ ·åˆ°äº†ä¸æ˜¯å¾ˆé«˜çš„åŠ¨ä½œåºåˆ—ã€‚é‚£ä¹ˆæˆ‘ä»¬å°±åªèƒ½åœ¨è¿™ä¸€ç¾¤çŸ®å­ä¸­æŒ‘é«˜çš„ï¼ŒæŠŠå‚æ•°è°ƒæ•´å¾—é è¿‘å…¶ä¸­æœ€é«˜çš„åŠ¨ä½œåºåˆ—ï¼Œæé«˜ä¸€ä¸ªåºåˆ—å‡ ç‡ä»£è¡¨ç€å‡å°‘å…¶ä»–åŠ¨ä½œåºåˆ—çš„å‡ ç‡ï¼Œæ‰€ä»¥é‚£ä¸ªåŸæœ¬å¥–åŠ±å€¼æœ€é«˜çš„${\tau}^{j}$ï¼Œå°±è¢«åŸ‹æ²¡äº†ã€‚**æ°”ä¸æ°”ï¼Ÿ**

æ‰€ä»¥æˆ‘ä»¬æå‡ºï¼ŒåŠ ä¸€æ¡åŸºçº¿ï¼Œè¿™æ ·çŸ®å­å°±ä¸ä¼šå› ä¸ºè¿æ°”å¥½ï¼Œæ²¡å’Œé«˜å­æ’åœ¨ä¸€èµ·è€Œè¢«å½“æˆé«˜å­å¤„ç†äº†ã€‚åŠ ä¸€æ¡åŸºçº¿ï¼Œåœ¨å…¬å¼ä¸­è¡¨ç¤ºä¸ºå¥–åŠ±å€¼å¤šå‡äº†ä¸€ä¸ª$b$ï¼š

$\nabla\overline{R_\theta}{\approx}\frac{1}{N}\sum^N_{n=1}\sum^{T_n}\_{t=1}(R(\tau^n)-b){\nabla}logp\_{\theta}(a^n_t|s^n_t)$

å…¶ä¸­è¿™ä¸ª$b$ï¼Œæˆ‘ä»¬ä¸€èˆ¬å–æ‰€æœ‰åºåˆ—æ€»å¥–åŠ±å€¼çš„æœŸæœ›ï¼š

$b {\approx}E[R({\tau})]$

### Tip2: Assign Suitable Credit

æˆ‘ä»¬çœ‹ä¸‹é¢è¿™ä¸ªä¾‹å­ï¼š

æˆ‘æœ‰ä¸¤ä¸ªepisodeï¼š

![image-20181210235905610](https://raw.githubusercontent.com/Shiyuang-scu/blog_img/master/policygradient-7.png)

æ ¹æ®å…¬å¼

$\nabla\overline{R_\theta}{\approx}\frac{1}{N}\sum^N_{n=1}\sum^{T_n}\_{t=1}(R(\tau^n)-b){\nabla}logp\_{\theta}(a^n_t|s^n_t)$

æˆ‘ä»¬è¦å¯¹æ¯ä¸€ä¸ªActionéƒ½ä¹˜ä¸Šç›¸åŒçš„Rï¼Œåœ¨episode1ä¸­æ˜¯3ï¼Œepisode2ä¸­ï¼Œæ˜¯7ã€‚

![image-20181211000035207](https://raw.githubusercontent.com/Shiyuang-scu/blog_img/master/policygradient-8.png)

å¾ˆæ˜¾ç„¶ï¼Œè¿™æ˜¯å¾ˆä¸å…¬å¹³çš„ã€‚å› ä¸ºåœ¨æœ€ç»ˆç»“æœå¥½çš„episodeé‡Œï¼ŒæŒ‡ä¸å‡†æœ‰ä¸€ä¸ªactionæ˜¯ä¸å¥½çš„ã€‚æ¯”å¦‚è¯´ä¸Šå›¾çš„episode1ï¼Œå®ƒçš„action2å¯¼è‡´äº†åæ¥action3çš„å¾—åˆ†ä¸ºè´Ÿã€‚ä½†æ˜¯æˆ‘ä»¬åŒæ ·æŠŠå®ƒçœ‹æˆäº†ä¸€ä¸ªå¥½çš„åŠ¨ä½œï¼Œå¹¶ä¸”å¢å¤§äº†è¿™ä¸ªåŠ¨ä½œçš„å‡ ç‡ã€‚

åŒæ ·çš„ï¼Œå³ä½¿åœ¨ç»“æœä¸å¥½çš„episodeé‡Œï¼Œä¹Ÿå¯èƒ½æœ‰å¥½çš„actionã€‚æ¯”å¦‚è¯´ä¸Šå›¾çš„episode2ï¼Œå®ƒçš„action2è™½ç„¶å¯¼è‡´äº†åæ¥action3çš„å¾—åˆ†ä¹Ÿä¸ºè´Ÿï¼Œä½†æ˜¯è´Ÿçš„ç¨‹åº¦å˜å°äº†ï¼Œè¯´æ˜action2è¿˜æ˜¯æœ‰ç›Šçš„ã€‚

é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æŠŠä»£æ›¿åŸæ¥çš„å…¬å¼ï¼š

$\nabla\overline{R_\theta}{\approx}\frac{1}{N}\sum^N_{n=1}\sum^{T_n}\_{t=1}(R(\tau^n)-b){\nabla}logp\_{\theta}(a^n_t|s^n_t)$

ä»£æ›¿ä¸ºä»¥ä¸‹å…¬å¼ï¼š

$\nabla\overline{R_\theta}{\approx}\frac{1}{N}\sum^N\_{n=1}\sum^{T_n}\_{t=1}({\sum\_{t'=t}^{T_n}r_{t'}^{n}}-b){\nabla}logp\_{\theta}(a^n_t|s^n_t)$

å…¶ä¸­ï¼Œæˆ‘ä»¬æŠŠ

$R(\tau^n)$

æ›¿æ¢æˆäº†

${\sum_{t'=t}^{T_n}r_{t'}^{n}}$

ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿæˆ‘ä»¬ç”±åŸæ¥çš„ï¼Œå¯¹åŠ¨ä½œ${a_i}$éƒ½ä¹˜ä¸Šä¸€ä¸ªç›¸åŒçš„Rï¼Œå˜æˆä¹˜ä¸Šä¸€ä¸ªä»¥åŠ¨ä½œ$a_i$ä¸ºèµ·ç‚¹çš„åŠ¨ä½œåºåˆ—çš„å¥–åŠ±å€¼ã€‚ç›´è§‚ä¸Šç†è§£ï¼Œå°±æ˜¯å¯¹æ¯ä¸ªåŠ¨ä½œçš„è°ƒèŠ‚ï¼Œä¸å†æ˜¯çœ‹æ•´ä¸ªåŠ¨ä½œåºåˆ—çš„ç»“æœäº†ï¼Œè€Œæ˜¯çœ‹è¿™ä¸ªåŠ¨ä½œåœ¨æ•´ä¸ªåŠ¨ä½œåºåˆ—ä¸­å‘æŒ¥çš„ä½œç”¨ã€‚

ç†è§£äº†ä¸Šé¢çš„æ„æ€ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå°†

${\sum_{t'=t}^{T_n}r_{t'}^{n}}$

æ”¹è¿›ä¸º

${\sum_{t'=t}^{T_n}{\gamma}^{t'-t}r_{t'}^{n}}$

å…¶ä¸­$0<{\gamma}<1$

è¿™ä¸ªä¹Ÿå¾ˆå¥½ç†è§£ï¼Œéšç€åŠ¨ä½œ$a_i$å’Œ$a_j$ä¹‹å‰éš”çš„æ—¶é—´è¶Šæ¥è¶Šä¹…ï¼Œä¸¤ä¸ªåŠ¨ä½œçš„ç›¸å…³æ€§å°±è¶Šæ¥è¶Šå°ã€‚




# Referenceï¼š

[Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)

http://karpathy.github.io/2016/05/31/rl/

*[Reinforcement learning An introduction](https://morvanzhou.github.io/static/files/Reinforcement_learning_An_introduction.pdf)*

*[Policy gradient methods for reinforcement learning with function approximation.](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)*

[â€œDeep Reinforcement Learning, 2018â€ by æå®æ¯… ](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html)













